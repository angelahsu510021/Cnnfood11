{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnnfoodtrain4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp5ZrzllDQ4m",
        "outputId": "462f30c9-d6c7-4992-9b85-e16ce59b6d31"
      },
      "source": [
        "!pip install torchsummary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuGSr4Ngd322"
      },
      "source": [
        "!gdown --id '10SYYurCRr4N-Lh_QW-YLifVRFQUZyluj' --output food-11.zip # 下載資料集\n",
        "!unzip food-11.zip # 解壓縮"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL23vbRpmYeE"
      },
      "source": [
        "\n",
        "利用 OpenCV (cv2) 讀入照片並存放在 numpy array 中\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rIJuK3zOP8h"
      },
      "source": [
        "# Import需要的套件\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import time\n",
        "from torchsummary import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THrIfj-iOVxU",
        "outputId": "c897b923-2380-4c04-aa60-b6459a06f39f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvWeuoA9OYjV"
      },
      "source": [
        "def readfile(path, label):\n",
        "    # label 是一個 boolean variable，代表需不需要回傳 y 值\n",
        "    image_dir = sorted(os.listdir(path))\n",
        "    x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8)\n",
        "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
        "    for i, file in enumerate(image_dir):\n",
        "        img = cv2.imread(os.path.join(path, file))\n",
        "        x[i, :, :] = cv2.resize(img,(128, 128))\n",
        "        if label:\n",
        "          y[i] = int(file.split(\"_\")[0])\n",
        "    if label:\n",
        "      return x, y\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIHWuhlZObN1",
        "outputId": "8699dec7-9ef5-4bc6-8488-b0a7bb920e14"
      },
      "source": [
        "# 分別將 training set、validation set、testing set 用 readfile 函式讀進來\n",
        "workspace_dir = './food-11'\n",
        "print(\"Reading data\")\n",
        "train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n",
        "print(\"Size of training data = {}\".format(len(train_x)))\n",
        "val_x, val_y = readfile(os.path.join(workspace_dir, \"validation\"), True)\n",
        "print(\"Size of validation data = {}\".format(len(val_x)))\n",
        "#test_x = readfile(os.path.join(workspace_dir, \"testing\"), False)\n",
        "#print(\"Size of Testing data = {}\".format(len(test_x)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading data\n",
            "Size of training data = 9866\n",
            "Size of validation data = 3430\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHfctVM5Oefs"
      },
      "source": [
        "# training 時做 data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomVerticalFlip(p=0.5),  # 隨機將圖片垂直翻轉\n",
        "    transforms.RandomHorizontalFlip(p=0.5), # 隨機將圖片水平翻轉\n",
        "    transforms.RandomRotation(20), # 隨機旋轉圖片\n",
        "    transforms.ColorJitter(), # 隨機色溫\n",
        "    transforms.RandomGrayscale(), #隨機灰階\n",
        "    transforms.ToTensor(),# 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
        "    #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),#參數待查看起來沒廢掉\n",
        "    #transforms.RandomPerspective(distortion_scale=0.3, p=0.5),\n",
        "    #transforms.RandomAffine(10),\n",
        "])\n",
        "# testing 時不需做 data augmentation\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),                                    \n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "class ImgDataset(Dataset):\n",
        "    def __init__(self, x, y=None, transform=None):\n",
        "        self.x = x\n",
        "        # label is required to be a LongTensor\n",
        "        self.y = y\n",
        "        if y is not None:\n",
        "            self.y = torch.LongTensor(y)\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    def __getitem__(self, index):\n",
        "        X = self.x[index]\n",
        "        if self.transform is not None:\n",
        "            X = self.transform(X)\n",
        "        if self.y is not None:\n",
        "            Y = self.y[index]\n",
        "            return X, Y\n",
        "        else:\n",
        "            return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UavTABVhHCr"
      },
      "source": [
        "batch_size = 64\n",
        "train_set = ImgDataset(train_x, train_y, train_transform)\n",
        "val_set = ImgDataset(val_x, val_y, test_transform)\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOchFptTOndb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2cffbee-5511-46ae-a38b-de2e752216de"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
        "        # input 維度 [3, 128, 128]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
        "\n",
        "            nn.Dropout2d(0.5),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
        "\n",
        "            nn.Dropout2d(0.3),\n",
        "            \n",
        "            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512*4*4, 1024),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Linear(256, 128),\n",
        "            nn.Linear(128, 11),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.cnn(x)\n",
        "        out = out.view(out.size()[0], -1)\n",
        "        return self.fc(out)\n",
        "\n",
        "        \n",
        "model=Classifier()\n",
        "summary(model.cuda(), (3, 128, 128))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 128, 128]           1,792\n",
            "       BatchNorm2d-2         [-1, 64, 128, 128]             128\n",
            "              ReLU-3         [-1, 64, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 64, 64, 64]               0\n",
            "            Conv2d-5          [-1, 128, 64, 64]          73,856\n",
            "       BatchNorm2d-6          [-1, 128, 64, 64]             256\n",
            "              ReLU-7          [-1, 128, 64, 64]               0\n",
            "         MaxPool2d-8          [-1, 128, 32, 32]               0\n",
            "         Dropout2d-9          [-1, 128, 32, 32]               0\n",
            "           Conv2d-10          [-1, 256, 32, 32]         295,168\n",
            "      BatchNorm2d-11          [-1, 256, 32, 32]             512\n",
            "             ReLU-12          [-1, 256, 32, 32]               0\n",
            "        MaxPool2d-13          [-1, 256, 16, 16]               0\n",
            "           Conv2d-14          [-1, 512, 16, 16]       1,180,160\n",
            "      BatchNorm2d-15          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-16          [-1, 512, 16, 16]               0\n",
            "        MaxPool2d-17            [-1, 512, 8, 8]               0\n",
            "        Dropout2d-18            [-1, 512, 8, 8]               0\n",
            "           Conv2d-19            [-1, 512, 8, 8]       2,359,808\n",
            "      BatchNorm2d-20            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-21            [-1, 512, 8, 8]               0\n",
            "        MaxPool2d-22            [-1, 512, 4, 4]               0\n",
            "           Linear-23                 [-1, 1024]       8,389,632\n",
            "             ReLU-24                 [-1, 1024]               0\n",
            "           Linear-25                  [-1, 512]         524,800\n",
            "             ReLU-26                  [-1, 512]               0\n",
            "           Linear-27                  [-1, 256]         131,328\n",
            "             ReLU-28                  [-1, 256]               0\n",
            "           Linear-29                  [-1, 128]          32,896\n",
            "           Linear-30                   [-1, 11]           1,419\n",
            "             ReLU-31                   [-1, 11]               0\n",
            "================================================================\n",
            "Total params: 12,993,803\n",
            "Trainable params: 12,993,803\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 50.84\n",
            "Params size (MB): 49.57\n",
            "Estimated Total Size (MB): 100.60\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdD4B6M-msJj"
      },
      "source": [
        "使用 training set 訓練，並使用 validation set 尋找好的參數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg2YDBteOsWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2690aa00-2806-40b6-c8d3-2457877f2033"
      },
      "source": [
        "model = Classifier().cuda()\n",
        "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer 使用 Adam\n",
        "num_epoch = 250\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    epoch_start_time = time.time()\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    model.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
        "        train_pred = model(data[0].cuda()) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
        "        batch_loss = loss(train_pred, data[1].cuda()) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
        "        batch_loss.backward() # 利用 back propagation 算出每個參數的 gradient\n",
        "        optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
        "\n",
        "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
        "        train_loss += batch_loss.item()\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "            val_pred = model(data[0].cuda())\n",
        "            batch_loss = loss(val_pred, data[1].cuda())\n",
        "\n",
        "            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
        "            val_loss += batch_loss.item()\n",
        "\n",
        "        #將結果 print 出來\n",
        "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
        "            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
        "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/250] 25.05 sec(s) Train Acc: 0.186398 Loss: 0.036563 | Val Acc: 0.207580 loss: 0.035562\n",
            "[002/250] 25.27 sec(s) Train Acc: 0.258058 Loss: 0.034431 | Val Acc: 0.294752 loss: 0.033422\n",
            "[003/250] 25.00 sec(s) Train Acc: 0.294648 Loss: 0.033201 | Val Acc: 0.279883 loss: 0.032839\n",
            "[004/250] 24.93 sec(s) Train Acc: 0.321305 Loss: 0.032018 | Val Acc: 0.354810 loss: 0.030731\n",
            "[005/250] 25.09 sec(s) Train Acc: 0.342489 Loss: 0.031192 | Val Acc: 0.354519 loss: 0.030463\n",
            "[006/250] 25.02 sec(s) Train Acc: 0.354551 Loss: 0.030660 | Val Acc: 0.328863 loss: 0.031314\n",
            "[007/250] 25.02 sec(s) Train Acc: 0.369045 Loss: 0.029780 | Val Acc: 0.304956 loss: 0.032146\n",
            "[008/250] 25.04 sec(s) Train Acc: 0.378269 Loss: 0.029405 | Val Acc: 0.395044 loss: 0.028666\n",
            "[009/250] 25.00 sec(s) Train Acc: 0.396716 Loss: 0.028614 | Val Acc: 0.391837 loss: 0.028803\n",
            "[010/250] 25.02 sec(s) Train Acc: 0.396108 Loss: 0.028256 | Val Acc: 0.418659 loss: 0.027549\n",
            "[011/250] 25.06 sec(s) Train Acc: 0.408474 Loss: 0.027857 | Val Acc: 0.430029 loss: 0.026939\n",
            "[012/250] 25.02 sec(s) Train Acc: 0.419319 Loss: 0.027487 | Val Acc: 0.403207 loss: 0.028563\n",
            "[013/250] 24.99 sec(s) Train Acc: 0.435029 Loss: 0.027009 | Val Acc: 0.418367 loss: 0.027690\n",
            "[014/250] 25.03 sec(s) Train Acc: 0.434320 Loss: 0.026830 | Val Acc: 0.438776 loss: 0.026624\n",
            "[015/250] 25.04 sec(s) Train Acc: 0.443645 Loss: 0.026448 | Val Acc: 0.437026 loss: 0.026910\n",
            "[016/250] 25.03 sec(s) Train Acc: 0.449524 Loss: 0.026172 | Val Acc: 0.428571 loss: 0.027462\n",
            "[017/250] 25.08 sec(s) Train Acc: 0.455098 Loss: 0.025900 | Val Acc: 0.481341 loss: 0.025077\n",
            "[018/250] 25.02 sec(s) Train Acc: 0.468275 Loss: 0.025344 | Val Acc: 0.472886 loss: 0.025039\n",
            "[019/250] 25.03 sec(s) Train Acc: 0.475269 Loss: 0.025225 | Val Acc: 0.464723 loss: 0.025807\n",
            "[020/250] 25.02 sec(s) Train Acc: 0.481046 Loss: 0.024682 | Val Acc: 0.495918 loss: 0.023305\n",
            "[021/250] 25.01 sec(s) Train Acc: 0.490371 Loss: 0.023332 | Val Acc: 0.501458 loss: 0.023023\n",
            "[022/250] 25.00 sec(s) Train Acc: 0.502433 Loss: 0.022675 | Val Acc: 0.507289 loss: 0.022098\n",
            "[023/250] 24.98 sec(s) Train Acc: 0.505271 Loss: 0.022284 | Val Acc: 0.517784 loss: 0.022344\n",
            "[024/250] 25.00 sec(s) Train Acc: 0.524225 Loss: 0.021884 | Val Acc: 0.515743 loss: 0.022556\n",
            "[025/250] 24.97 sec(s) Train Acc: 0.526454 Loss: 0.021351 | Val Acc: 0.516910 loss: 0.021856\n",
            "[026/250] 24.95 sec(s) Train Acc: 0.535577 Loss: 0.021105 | Val Acc: 0.539650 loss: 0.021190\n",
            "[027/250] 25.02 sec(s) Train Acc: 0.540543 Loss: 0.020790 | Val Acc: 0.526531 loss: 0.022060\n",
            "[028/250] 25.07 sec(s) Train Acc: 0.543179 Loss: 0.020729 | Val Acc: 0.557143 loss: 0.020174\n",
            "[029/250] 25.00 sec(s) Train Acc: 0.553112 Loss: 0.020310 | Val Acc: 0.567347 loss: 0.020024\n",
            "[030/250] 25.01 sec(s) Train Acc: 0.554125 Loss: 0.020167 | Val Acc: 0.567055 loss: 0.020408\n",
            "[031/250] 25.02 sec(s) Train Acc: 0.554936 Loss: 0.019928 | Val Acc: 0.554810 loss: 0.020712\n",
            "[032/250] 24.99 sec(s) Train Acc: 0.562538 Loss: 0.019759 | Val Acc: 0.545773 loss: 0.020810\n",
            "[033/250] 25.04 sec(s) Train Acc: 0.571153 Loss: 0.019553 | Val Acc: 0.518076 loss: 0.022376\n",
            "[034/250] 25.05 sec(s) Train Acc: 0.574194 Loss: 0.019515 | Val Acc: 0.572886 loss: 0.019681\n",
            "[035/250] 24.96 sec(s) Train Acc: 0.578654 Loss: 0.018985 | Val Acc: 0.562099 loss: 0.020249\n",
            "[036/250] 24.95 sec(s) Train Acc: 0.587979 Loss: 0.018666 | Val Acc: 0.567347 loss: 0.020039\n",
            "[037/250] 25.00 sec(s) Train Acc: 0.594060 Loss: 0.018354 | Val Acc: 0.594752 loss: 0.018912\n",
            "[038/250] 24.99 sec(s) Train Acc: 0.590716 Loss: 0.018428 | Val Acc: 0.558601 loss: 0.020098\n",
            "[039/250] 25.02 sec(s) Train Acc: 0.597000 Loss: 0.018266 | Val Acc: 0.571137 loss: 0.019866\n",
            "[040/250] 24.99 sec(s) Train Acc: 0.603183 Loss: 0.017963 | Val Acc: 0.565598 loss: 0.020294\n",
            "[041/250] 25.03 sec(s) Train Acc: 0.603284 Loss: 0.017848 | Val Acc: 0.585131 loss: 0.019215\n",
            "[042/250] 24.97 sec(s) Train Acc: 0.618893 Loss: 0.017404 | Val Acc: 0.567930 loss: 0.020040\n",
            "[043/250] 24.98 sec(s) Train Acc: 0.614231 Loss: 0.017396 | Val Acc: 0.581633 loss: 0.019641\n",
            "[044/250] 25.04 sec(s) Train Acc: 0.614636 Loss: 0.017326 | Val Acc: 0.567347 loss: 0.020676\n",
            "[045/250] 24.97 sec(s) Train Acc: 0.621326 Loss: 0.017065 | Val Acc: 0.604956 loss: 0.018469\n",
            "[046/250] 25.03 sec(s) Train Acc: 0.627103 Loss: 0.016790 | Val Acc: 0.581633 loss: 0.019208\n",
            "[047/250] 25.01 sec(s) Train Acc: 0.625076 Loss: 0.016835 | Val Acc: 0.593003 loss: 0.018652\n",
            "[048/250] 24.99 sec(s) Train Acc: 0.629738 Loss: 0.016783 | Val Acc: 0.584257 loss: 0.019713\n",
            "[049/250] 24.95 sec(s) Train Acc: 0.637644 Loss: 0.016585 | Val Acc: 0.618659 loss: 0.018213\n",
            "[050/250] 25.00 sec(s) Train Acc: 0.644841 Loss: 0.016021 | Val Acc: 0.590379 loss: 0.019925\n",
            "[051/250] 24.99 sec(s) Train Acc: 0.653152 Loss: 0.015817 | Val Acc: 0.581924 loss: 0.020603\n",
            "[052/250] 24.98 sec(s) Train Acc: 0.648186 Loss: 0.016023 | Val Acc: 0.609621 loss: 0.019026\n",
            "[053/250] 25.08 sec(s) Train Acc: 0.654774 Loss: 0.015778 | Val Acc: 0.619534 loss: 0.018204\n",
            "[054/250] 25.06 sec(s) Train Acc: 0.663187 Loss: 0.015441 | Val Acc: 0.626239 loss: 0.017873\n",
            "[055/250] 25.01 sec(s) Train Acc: 0.660450 Loss: 0.015407 | Val Acc: 0.582216 loss: 0.020411\n",
            "[056/250] 25.01 sec(s) Train Acc: 0.665923 Loss: 0.015436 | Val Acc: 0.550437 loss: 0.023255\n",
            "[057/250] 25.02 sec(s) Train Acc: 0.662072 Loss: 0.015076 | Val Acc: 0.620991 loss: 0.017801\n",
            "[058/250] 24.99 sec(s) Train Acc: 0.681228 Loss: 0.014702 | Val Acc: 0.620700 loss: 0.017670\n",
            "[059/250] 25.00 sec(s) Train Acc: 0.676870 Loss: 0.014736 | Val Acc: 0.608746 loss: 0.019043\n",
            "[060/250] 25.05 sec(s) Train Acc: 0.680519 Loss: 0.014519 | Val Acc: 0.604956 loss: 0.019759\n",
            "[061/250] 25.04 sec(s) Train Acc: 0.682343 Loss: 0.014524 | Val Acc: 0.621866 loss: 0.018455\n",
            "[062/250] 24.99 sec(s) Train Acc: 0.686803 Loss: 0.014193 | Val Acc: 0.626822 loss: 0.018195\n",
            "[063/250] 25.03 sec(s) Train Acc: 0.694506 Loss: 0.014021 | Val Acc: 0.625073 loss: 0.017872\n",
            "[064/250] 24.96 sec(s) Train Acc: 0.697446 Loss: 0.013748 | Val Acc: 0.636735 loss: 0.017430\n",
            "[065/250] 24.98 sec(s) Train Acc: 0.693189 Loss: 0.013951 | Val Acc: 0.620408 loss: 0.018873\n",
            "[066/250] 25.04 sec(s) Train Acc: 0.703122 Loss: 0.013615 | Val Acc: 0.618950 loss: 0.018706\n",
            "[067/250] 25.00 sec(s) Train Acc: 0.706568 Loss: 0.013315 | Val Acc: 0.631487 loss: 0.017637\n",
            "[068/250] 24.99 sec(s) Train Acc: 0.699777 Loss: 0.013545 | Val Acc: 0.637609 loss: 0.017496\n",
            "[069/250] 25.02 sec(s) Train Acc: 0.703020 Loss: 0.013240 | Val Acc: 0.625656 loss: 0.018169\n",
            "[070/250] 24.99 sec(s) Train Acc: 0.705352 Loss: 0.013186 | Val Acc: 0.630321 loss: 0.018166\n",
            "[071/250] 24.94 sec(s) Train Acc: 0.713359 Loss: 0.013063 | Val Acc: 0.618659 loss: 0.019739\n",
            "[072/250] 25.00 sec(s) Train Acc: 0.715488 Loss: 0.012950 | Val Acc: 0.639650 loss: 0.017699\n",
            "[073/250] 25.01 sec(s) Train Acc: 0.715792 Loss: 0.012711 | Val Acc: 0.629155 loss: 0.017812\n",
            "[074/250] 24.97 sec(s) Train Acc: 0.734239 Loss: 0.012255 | Val Acc: 0.621283 loss: 0.019719\n",
            "[075/250] 24.93 sec(s) Train Acc: 0.727245 Loss: 0.012290 | Val Acc: 0.638192 loss: 0.018498\n",
            "[076/250] 25.03 sec(s) Train Acc: 0.732009 Loss: 0.012209 | Val Acc: 0.634985 loss: 0.018508\n",
            "[077/250] 25.00 sec(s) Train Acc: 0.731603 Loss: 0.012141 | Val Acc: 0.625364 loss: 0.019191\n",
            "[078/250] 24.95 sec(s) Train Acc: 0.739611 Loss: 0.012076 | Val Acc: 0.627988 loss: 0.018712\n",
            "[079/250] 25.01 sec(s) Train Acc: 0.744983 Loss: 0.011660 | Val Acc: 0.646647 loss: 0.017904\n",
            "[080/250] 24.95 sec(s) Train Acc: 0.746199 Loss: 0.011734 | Val Acc: 0.648397 loss: 0.017830\n",
            "[081/250] 24.92 sec(s) Train Acc: 0.747821 Loss: 0.011498 | Val Acc: 0.637026 loss: 0.018267\n",
            "[082/250] 25.00 sec(s) Train Acc: 0.744476 Loss: 0.011525 | Val Acc: 0.648688 loss: 0.017994\n",
            "[083/250] 24.98 sec(s) Train Acc: 0.762214 Loss: 0.011180 | Val Acc: 0.621866 loss: 0.020506\n",
            "[084/250] 25.07 sec(s) Train Acc: 0.752281 Loss: 0.011224 | Val Acc: 0.651312 loss: 0.018063\n",
            "[085/250] 25.07 sec(s) Train Acc: 0.757551 Loss: 0.011202 | Val Acc: 0.625948 loss: 0.019121\n",
            "[086/250] 25.17 sec(s) Train Acc: 0.759477 Loss: 0.010915 | Val Acc: 0.629155 loss: 0.019080\n",
            "[087/250] 25.02 sec(s) Train Acc: 0.759274 Loss: 0.011005 | Val Acc: 0.614869 loss: 0.019605\n",
            "[088/250] 24.99 sec(s) Train Acc: 0.767180 Loss: 0.010565 | Val Acc: 0.633819 loss: 0.019249\n",
            "[089/250] 25.02 sec(s) Train Acc: 0.769917 Loss: 0.010529 | Val Acc: 0.636152 loss: 0.019246\n",
            "[090/250] 25.02 sec(s) Train Acc: 0.774073 Loss: 0.010428 | Val Acc: 0.629155 loss: 0.019697\n",
            "[091/250] 25.02 sec(s) Train Acc: 0.778330 Loss: 0.010133 | Val Acc: 0.652478 loss: 0.018018\n",
            "[092/250] 25.09 sec(s) Train Acc: 0.775796 Loss: 0.010297 | Val Acc: 0.647813 loss: 0.018549\n",
            "[093/250] 24.98 sec(s) Train Acc: 0.775796 Loss: 0.010344 | Val Acc: 0.643149 loss: 0.019092\n",
            "[094/250] 24.98 sec(s) Train Acc: 0.781979 Loss: 0.009861 | Val Acc: 0.642857 loss: 0.019002\n",
            "[095/250] 25.02 sec(s) Train Acc: 0.787857 Loss: 0.009689 | Val Acc: 0.649854 loss: 0.018455\n",
            "[096/250] 24.99 sec(s) Train Acc: 0.788770 Loss: 0.009549 | Val Acc: 0.623324 loss: 0.021051\n",
            "[097/250] 25.00 sec(s) Train Acc: 0.784918 Loss: 0.009848 | Val Acc: 0.627405 loss: 0.019899\n",
            "[098/250] 25.06 sec(s) Train Acc: 0.792114 Loss: 0.009382 | Val Acc: 0.637609 loss: 0.019793\n",
            "[099/250] 25.02 sec(s) Train Acc: 0.795763 Loss: 0.009174 | Val Acc: 0.645773 loss: 0.019401\n",
            "[100/250] 24.97 sec(s) Train Acc: 0.802149 Loss: 0.009121 | Val Acc: 0.646064 loss: 0.018987\n",
            "[101/250] 25.12 sec(s) Train Acc: 0.796473 Loss: 0.009187 | Val Acc: 0.652187 loss: 0.019365\n",
            "[102/250] 25.12 sec(s) Train Acc: 0.797182 Loss: 0.009285 | Val Acc: 0.634402 loss: 0.020835\n",
            "[103/250] 25.08 sec(s) Train Acc: 0.796067 Loss: 0.009305 | Val Acc: 0.647813 loss: 0.018836\n",
            "[104/250] 25.05 sec(s) Train Acc: 0.803061 Loss: 0.009024 | Val Acc: 0.647230 loss: 0.019244\n",
            "[105/250] 25.12 sec(s) Train Acc: 0.804480 Loss: 0.008933 | Val Acc: 0.644023 loss: 0.019705\n",
            "[106/250] 25.14 sec(s) Train Acc: 0.808636 Loss: 0.008731 | Val Acc: 0.633528 loss: 0.020401\n",
            "[107/250] 25.12 sec(s) Train Acc: 0.817150 Loss: 0.008607 | Val Acc: 0.645190 loss: 0.020142\n",
            "[108/250] 25.15 sec(s) Train Acc: 0.816035 Loss: 0.008480 | Val Acc: 0.621283 loss: 0.021406\n",
            "[109/250] 25.17 sec(s) Train Acc: 0.816542 Loss: 0.008477 | Val Acc: 0.627988 loss: 0.021390\n",
            "[110/250] 25.11 sec(s) Train Acc: 0.824448 Loss: 0.008080 | Val Acc: 0.642566 loss: 0.019493\n",
            "[111/250] 25.17 sec(s) Train Acc: 0.822826 Loss: 0.008083 | Val Acc: 0.640816 loss: 0.020215\n",
            "[112/250] 25.13 sec(s) Train Acc: 0.824346 Loss: 0.008019 | Val Acc: 0.644898 loss: 0.020376\n",
            "[113/250] 25.13 sec(s) Train Acc: 0.817657 Loss: 0.008318 | Val Acc: 0.646647 loss: 0.019507\n",
            "[114/250] 25.16 sec(s) Train Acc: 0.820596 Loss: 0.008046 | Val Acc: 0.662974 loss: 0.018939\n",
            "[115/250] 25.21 sec(s) Train Acc: 0.830935 Loss: 0.007831 | Val Acc: 0.653061 loss: 0.019841\n",
            "[116/250] 25.13 sec(s) Train Acc: 0.828705 Loss: 0.007795 | Val Acc: 0.641108 loss: 0.020490\n",
            "[117/250] 25.20 sec(s) Train Acc: 0.830833 Loss: 0.007978 | Val Acc: 0.625948 loss: 0.021907\n",
            "[118/250] 25.17 sec(s) Train Acc: 0.825258 Loss: 0.008006 | Val Acc: 0.648688 loss: 0.019166\n",
            "[119/250] 25.22 sec(s) Train Acc: 0.834381 Loss: 0.007482 | Val Acc: 0.650729 loss: 0.019952\n",
            "[120/250] 25.28 sec(s) Train Acc: 0.830326 Loss: 0.007621 | Val Acc: 0.649854 loss: 0.020318\n",
            "[121/250] 25.13 sec(s) Train Acc: 0.836205 Loss: 0.007561 | Val Acc: 0.646064 loss: 0.021000\n",
            "[122/250] 25.13 sec(s) Train Acc: 0.835090 Loss: 0.007493 | Val Acc: 0.656560 loss: 0.020307\n",
            "[123/250] 25.10 sec(s) Train Acc: 0.842591 Loss: 0.007199 | Val Acc: 0.626531 loss: 0.022019\n",
            "[124/250] 25.18 sec(s) Train Acc: 0.845631 Loss: 0.007058 | Val Acc: 0.641983 loss: 0.021041\n",
            "[125/250] 25.15 sec(s) Train Acc: 0.844415 Loss: 0.007052 | Val Acc: 0.645481 loss: 0.021388\n",
            "[126/250] 25.05 sec(s) Train Acc: 0.840969 Loss: 0.007185 | Val Acc: 0.651312 loss: 0.019406\n",
            "[127/250] 25.12 sec(s) Train Acc: 0.847355 Loss: 0.007045 | Val Acc: 0.647813 loss: 0.021419\n",
            "[128/250] 25.04 sec(s) Train Acc: 0.851612 Loss: 0.006872 | Val Acc: 0.653353 loss: 0.020072\n",
            "[129/250] 25.00 sec(s) Train Acc: 0.849382 Loss: 0.006844 | Val Acc: 0.639067 loss: 0.021837\n",
            "[130/250] 24.99 sec(s) Train Acc: 0.855666 Loss: 0.006762 | Val Acc: 0.653353 loss: 0.021429\n",
            "[131/250] 25.02 sec(s) Train Acc: 0.849787 Loss: 0.006934 | Val Acc: 0.648105 loss: 0.020946\n",
            "[132/250] 24.99 sec(s) Train Acc: 0.859518 Loss: 0.006691 | Val Acc: 0.654519 loss: 0.021258\n",
            "[133/250] 24.98 sec(s) Train Acc: 0.852929 Loss: 0.006708 | Val Acc: 0.651603 loss: 0.020639\n",
            "[134/250] 25.04 sec(s) Train Acc: 0.856274 Loss: 0.006690 | Val Acc: 0.658892 loss: 0.020639\n",
            "[135/250] 24.97 sec(s) Train Acc: 0.864281 Loss: 0.006390 | Val Acc: 0.652187 loss: 0.021432\n",
            "[136/250] 25.00 sec(s) Train Acc: 0.864585 Loss: 0.006302 | Val Acc: 0.630904 loss: 0.022593\n",
            "[137/250] 25.03 sec(s) Train Acc: 0.868640 Loss: 0.006123 | Val Acc: 0.652187 loss: 0.020659\n",
            "[138/250] 24.93 sec(s) Train Acc: 0.859213 Loss: 0.006557 | Val Acc: 0.644315 loss: 0.022220\n",
            "[139/250] 25.00 sec(s) Train Acc: 0.863471 Loss: 0.006336 | Val Acc: 0.655102 loss: 0.021702\n",
            "[140/250] 25.00 sec(s) Train Acc: 0.865194 Loss: 0.006068 | Val Acc: 0.639359 loss: 0.022369\n",
            "[141/250] 25.06 sec(s) Train Acc: 0.864687 Loss: 0.006283 | Val Acc: 0.662682 loss: 0.021395\n",
            "[142/250] 24.99 sec(s) Train Acc: 0.871985 Loss: 0.005944 | Val Acc: 0.653644 loss: 0.021609\n",
            "[143/250] 24.97 sec(s) Train Acc: 0.868336 Loss: 0.006075 | Val Acc: 0.665015 loss: 0.020986\n",
            "[144/250] 25.01 sec(s) Train Acc: 0.869248 Loss: 0.005920 | Val Acc: 0.655102 loss: 0.021178\n",
            "[145/250] 24.97 sec(s) Train Acc: 0.866410 Loss: 0.006227 | Val Acc: 0.646939 loss: 0.020700\n",
            "[146/250] 24.99 sec(s) Train Acc: 0.874620 Loss: 0.005817 | Val Acc: 0.648688 loss: 0.022237\n",
            "[147/250] 24.98 sec(s) Train Acc: 0.871376 Loss: 0.006041 | Val Acc: 0.657143 loss: 0.021579\n",
            "[148/250] 24.99 sec(s) Train Acc: 0.873302 Loss: 0.005839 | Val Acc: 0.656560 loss: 0.021336\n",
            "[149/250] 24.97 sec(s) Train Acc: 0.876951 Loss: 0.005608 | Val Acc: 0.646647 loss: 0.022390\n",
            "[150/250] 24.95 sec(s) Train Acc: 0.877762 Loss: 0.005559 | Val Acc: 0.660350 loss: 0.021078\n",
            "[151/250] 24.91 sec(s) Train Acc: 0.874214 Loss: 0.005814 | Val Acc: 0.651312 loss: 0.022304\n",
            "[152/250] 24.90 sec(s) Train Acc: 0.878776 Loss: 0.005623 | Val Acc: 0.648105 loss: 0.021705\n",
            "[153/250] 25.03 sec(s) Train Acc: 0.878370 Loss: 0.005621 | Val Acc: 0.655977 loss: 0.021632\n",
            "[154/250] 25.01 sec(s) Train Acc: 0.881816 Loss: 0.005480 | Val Acc: 0.653936 loss: 0.022262\n",
            "[155/250] 25.02 sec(s) Train Acc: 0.878573 Loss: 0.005661 | Val Acc: 0.668805 loss: 0.021042\n",
            "[156/250] 25.04 sec(s) Train Acc: 0.890330 Loss: 0.005187 | Val Acc: 0.660350 loss: 0.022831\n",
            "[157/250] 25.11 sec(s) Train Acc: 0.888810 Loss: 0.005219 | Val Acc: 0.646647 loss: 0.022984\n",
            "[158/250] 25.12 sec(s) Train Acc: 0.884452 Loss: 0.005404 | Val Acc: 0.669971 loss: 0.021127\n",
            "[159/250] 25.00 sec(s) Train Acc: 0.890837 Loss: 0.005074 | Val Acc: 0.651895 loss: 0.022478\n",
            "[160/250] 25.04 sec(s) Train Acc: 0.883641 Loss: 0.005255 | Val Acc: 0.655394 loss: 0.022975\n",
            "[161/250] 24.99 sec(s) Train Acc: 0.884452 Loss: 0.005338 | Val Acc: 0.644606 loss: 0.023065\n",
            "[162/250] 24.96 sec(s) Train Acc: 0.892966 Loss: 0.004915 | Val Acc: 0.646647 loss: 0.022983\n",
            "[163/250] 25.04 sec(s) Train Acc: 0.892662 Loss: 0.004930 | Val Acc: 0.656560 loss: 0.023151\n",
            "[164/250] 24.94 sec(s) Train Acc: 0.892459 Loss: 0.005209 | Val Acc: 0.653936 loss: 0.022328\n",
            "[165/250] 25.01 sec(s) Train Acc: 0.893574 Loss: 0.005038 | Val Acc: 0.651312 loss: 0.022121\n",
            "[166/250] 25.05 sec(s) Train Acc: 0.899757 Loss: 0.004826 | Val Acc: 0.650146 loss: 0.023377\n",
            "[167/250] 24.96 sec(s) Train Acc: 0.899554 Loss: 0.004828 | Val Acc: 0.656560 loss: 0.022567\n",
            "[168/250] 25.01 sec(s) Train Acc: 0.900466 Loss: 0.004600 | Val Acc: 0.650729 loss: 0.023375\n",
            "[169/250] 24.98 sec(s) Train Acc: 0.897628 Loss: 0.004903 | Val Acc: 0.647230 loss: 0.023925\n",
            "[170/250] 24.98 sec(s) Train Acc: 0.895297 Loss: 0.004900 | Val Acc: 0.651895 loss: 0.023021\n",
            "[171/250] 25.03 sec(s) Train Acc: 0.898642 Loss: 0.004976 | Val Acc: 0.659184 loss: 0.022566\n",
            "[172/250] 24.95 sec(s) Train Acc: 0.899655 Loss: 0.004646 | Val Acc: 0.661516 loss: 0.021914\n",
            "[173/250] 24.97 sec(s) Train Acc: 0.894892 Loss: 0.004994 | Val Acc: 0.667055 loss: 0.021869\n",
            "[174/250] 24.88 sec(s) Train Acc: 0.896108 Loss: 0.004969 | Val Acc: 0.651895 loss: 0.022930\n",
            "[175/250] 24.91 sec(s) Train Acc: 0.894283 Loss: 0.004778 | Val Acc: 0.647522 loss: 0.023299\n",
            "[176/250] 25.03 sec(s) Train Acc: 0.901784 Loss: 0.004617 | Val Acc: 0.659767 loss: 0.022488\n",
            "[177/250] 25.08 sec(s) Train Acc: 0.907359 Loss: 0.004358 | Val Acc: 0.617784 loss: 0.027188\n",
            "[178/250] 25.11 sec(s) Train Acc: 0.907865 Loss: 0.004367 | Val Acc: 0.669388 loss: 0.021873\n",
            "[179/250] 25.06 sec(s) Train Acc: 0.900162 Loss: 0.004621 | Val Acc: 0.656851 loss: 0.023257\n",
            "[180/250] 25.07 sec(s) Train Acc: 0.910095 Loss: 0.004307 | Val Acc: 0.651603 loss: 0.024602\n",
            "[181/250] 25.04 sec(s) Train Acc: 0.899351 Loss: 0.004856 | Val Acc: 0.656851 loss: 0.022550\n",
            "[182/250] 25.03 sec(s) Train Acc: 0.909994 Loss: 0.004356 | Val Acc: 0.657434 loss: 0.023034\n",
            "[183/250] 25.01 sec(s) Train Acc: 0.905940 Loss: 0.004451 | Val Acc: 0.659767 loss: 0.021625\n",
            "[184/250] 24.99 sec(s) Train Acc: 0.908980 Loss: 0.004391 | Val Acc: 0.655102 loss: 0.023380\n",
            "[185/250] 24.99 sec(s) Train Acc: 0.908372 Loss: 0.004347 | Val Acc: 0.658017 loss: 0.022627\n",
            "[186/250] 25.09 sec(s) Train Acc: 0.906548 Loss: 0.004391 | Val Acc: 0.665015 loss: 0.023025\n",
            "[187/250] 25.03 sec(s) Train Acc: 0.909588 Loss: 0.004199 | Val Acc: 0.657143 loss: 0.023390\n",
            "[188/250] 24.99 sec(s) Train Acc: 0.905230 Loss: 0.004474 | Val Acc: 0.651895 loss: 0.022657\n",
            "[189/250] 24.99 sec(s) Train Acc: 0.905838 Loss: 0.004410 | Val Acc: 0.654519 loss: 0.023693\n",
            "[190/250] 25.01 sec(s) Train Acc: 0.910906 Loss: 0.004184 | Val Acc: 0.646064 loss: 0.023131\n",
            "[191/250] 25.02 sec(s) Train Acc: 0.911920 Loss: 0.004092 | Val Acc: 0.629446 loss: 0.025871\n",
            "[192/250] 25.07 sec(s) Train Acc: 0.908879 Loss: 0.004275 | Val Acc: 0.662974 loss: 0.022758\n",
            "[193/250] 25.01 sec(s) Train Acc: 0.917089 Loss: 0.004065 | Val Acc: 0.652478 loss: 0.024576\n",
            "[194/250] 24.98 sec(s) Train Acc: 0.915974 Loss: 0.003937 | Val Acc: 0.648105 loss: 0.023976\n",
            "[195/250] 25.03 sec(s) Train Acc: 0.918609 Loss: 0.003834 | Val Acc: 0.655685 loss: 0.023891\n",
            "[196/250] 25.01 sec(s) Train Acc: 0.914960 Loss: 0.003950 | Val Acc: 0.654810 loss: 0.024094\n",
            "[197/250] 25.00 sec(s) Train Acc: 0.910805 Loss: 0.004176 | Val Acc: 0.636443 loss: 0.023834\n",
            "[198/250] 24.89 sec(s) Train Acc: 0.917190 Loss: 0.004003 | Val Acc: 0.648688 loss: 0.023829\n",
            "[199/250] 24.94 sec(s) Train Acc: 0.915062 Loss: 0.004076 | Val Acc: 0.655102 loss: 0.024195\n",
            "[200/250] 24.96 sec(s) Train Acc: 0.910197 Loss: 0.004122 | Val Acc: 0.661224 loss: 0.023284\n",
            "[201/250] 24.95 sec(s) Train Acc: 0.920941 Loss: 0.003695 | Val Acc: 0.657726 loss: 0.025283\n",
            "[202/250] 24.96 sec(s) Train Acc: 0.915163 Loss: 0.003921 | Val Acc: 0.656851 loss: 0.023421\n",
            "[203/250] 24.95 sec(s) Train Acc: 0.917190 Loss: 0.003847 | Val Acc: 0.643732 loss: 0.024409\n",
            "[204/250] 24.88 sec(s) Train Acc: 0.920332 Loss: 0.003812 | Val Acc: 0.665015 loss: 0.023074\n",
            "[205/250] 24.91 sec(s) Train Acc: 0.921650 Loss: 0.003647 | Val Acc: 0.649854 loss: 0.023305\n",
            "[206/250] 24.92 sec(s) Train Acc: 0.917798 Loss: 0.003881 | Val Acc: 0.654810 loss: 0.023880\n",
            "[207/250] 24.86 sec(s) Train Acc: 0.922765 Loss: 0.003649 | Val Acc: 0.652770 loss: 0.024730\n",
            "[208/250] 24.89 sec(s) Train Acc: 0.917494 Loss: 0.003892 | Val Acc: 0.651603 loss: 0.024483\n",
            "[209/250] 24.95 sec(s) Train Acc: 0.923272 Loss: 0.003601 | Val Acc: 0.660641 loss: 0.023696\n",
            "[210/250] 24.97 sec(s) Train Acc: 0.918305 Loss: 0.003920 | Val Acc: 0.660933 loss: 0.023599\n",
            "[211/250] 25.07 sec(s) Train Acc: 0.918001 Loss: 0.003975 | Val Acc: 0.662682 loss: 0.023866\n",
            "[212/250] 25.28 sec(s) Train Acc: 0.920130 Loss: 0.003790 | Val Acc: 0.660933 loss: 0.024492\n",
            "[213/250] 25.06 sec(s) Train Acc: 0.930266 Loss: 0.003368 | Val Acc: 0.658601 loss: 0.025556\n",
            "[214/250] 24.95 sec(s) Train Acc: 0.921447 Loss: 0.003730 | Val Acc: 0.662974 loss: 0.023388\n",
            "[215/250] 25.06 sec(s) Train Acc: 0.922562 Loss: 0.003672 | Val Acc: 0.658017 loss: 0.024403\n",
            "[216/250] 25.16 sec(s) Train Acc: 0.922056 Loss: 0.003659 | Val Acc: 0.668805 loss: 0.023023\n",
            "[217/250] 25.05 sec(s) Train Acc: 0.927833 Loss: 0.003492 | Val Acc: 0.643149 loss: 0.025837\n",
            "[218/250] 25.01 sec(s) Train Acc: 0.922765 Loss: 0.003616 | Val Acc: 0.662682 loss: 0.024638\n",
            "[219/250] 24.99 sec(s) Train Acc: 0.920535 Loss: 0.003817 | Val Acc: 0.664431 loss: 0.024539\n",
            "[220/250] 24.91 sec(s) Train Acc: 0.928238 Loss: 0.003466 | Val Acc: 0.669971 loss: 0.022761\n",
            "[221/250] 24.90 sec(s) Train Acc: 0.924995 Loss: 0.003562 | Val Acc: 0.660933 loss: 0.024397\n",
            "[222/250] 24.91 sec(s) Train Acc: 0.924995 Loss: 0.003542 | Val Acc: 0.658017 loss: 0.024031\n",
            "[223/250] 24.93 sec(s) Train Acc: 0.921143 Loss: 0.003648 | Val Acc: 0.662974 loss: 0.023307\n",
            "[224/250] 24.94 sec(s) Train Acc: 0.928847 Loss: 0.003306 | Val Acc: 0.665015 loss: 0.024211\n",
            "[225/250] 24.91 sec(s) Train Acc: 0.925907 Loss: 0.003514 | Val Acc: 0.653644 loss: 0.025138\n",
            "[226/250] 24.93 sec(s) Train Acc: 0.926515 Loss: 0.003486 | Val Acc: 0.658017 loss: 0.024998\n",
            "[227/250] 24.94 sec(s) Train Acc: 0.928948 Loss: 0.003377 | Val Acc: 0.658309 loss: 0.023455\n",
            "[228/250] 24.90 sec(s) Train Acc: 0.929353 Loss: 0.003350 | Val Acc: 0.658017 loss: 0.024943\n",
            "[229/250] 24.82 sec(s) Train Acc: 0.927225 Loss: 0.003300 | Val Acc: 0.659184 loss: 0.024717\n",
            "[230/250] 24.93 sec(s) Train Acc: 0.933813 Loss: 0.003260 | Val Acc: 0.656851 loss: 0.026781\n",
            "[231/250] 24.88 sec(s) Train Acc: 0.933002 Loss: 0.003261 | Val Acc: 0.657434 loss: 0.025519\n",
            "[232/250] 24.81 sec(s) Train Acc: 0.927934 Loss: 0.003508 | Val Acc: 0.661516 loss: 0.024673\n",
            "[233/250] 24.89 sec(s) Train Acc: 0.931887 Loss: 0.003172 | Val Acc: 0.638776 loss: 0.027626\n",
            "[234/250] 24.90 sec(s) Train Acc: 0.926819 Loss: 0.003500 | Val Acc: 0.652187 loss: 0.025847\n",
            "[235/250] 24.97 sec(s) Train Acc: 0.931786 Loss: 0.003348 | Val Acc: 0.655102 loss: 0.024478\n",
            "[236/250] 25.00 sec(s) Train Acc: 0.932901 Loss: 0.003119 | Val Acc: 0.669679 loss: 0.023639\n",
            "[237/250] 25.08 sec(s) Train Acc: 0.934624 Loss: 0.003067 | Val Acc: 0.656851 loss: 0.024921\n",
            "[238/250] 25.08 sec(s) Train Acc: 0.932191 Loss: 0.003207 | Val Acc: 0.658309 loss: 0.024636\n",
            "[239/250] 24.94 sec(s) Train Acc: 0.931989 Loss: 0.003182 | Val Acc: 0.660058 loss: 0.024070\n",
            "[240/250] 24.89 sec(s) Train Acc: 0.939185 Loss: 0.002930 | Val Acc: 0.652770 loss: 0.025547\n",
            "[241/250] 24.95 sec(s) Train Acc: 0.932495 Loss: 0.003169 | Val Acc: 0.654519 loss: 0.024980\n",
            "[242/250] 24.91 sec(s) Train Acc: 0.928847 Loss: 0.003371 | Val Acc: 0.670262 loss: 0.023960\n",
            "[243/250] 24.99 sec(s) Train Acc: 0.928036 Loss: 0.003310 | Val Acc: 0.661516 loss: 0.023996\n",
            "[244/250] 25.38 sec(s) Train Acc: 0.935536 Loss: 0.002987 | Val Acc: 0.660058 loss: 0.024458\n",
            "[245/250] 25.23 sec(s) Train Acc: 0.932597 Loss: 0.003167 | Val Acc: 0.669679 loss: 0.023401\n",
            "[246/250] 25.03 sec(s) Train Acc: 0.930266 Loss: 0.003159 | Val Acc: 0.668222 loss: 0.025501\n",
            "[247/250] 25.06 sec(s) Train Acc: 0.932394 Loss: 0.003261 | Val Acc: 0.658309 loss: 0.023678\n",
            "[248/250] 24.96 sec(s) Train Acc: 0.935942 Loss: 0.003009 | Val Acc: 0.671720 loss: 0.024023\n",
            "[249/250] 24.92 sec(s) Train Acc: 0.935333 Loss: 0.003216 | Val Acc: 0.670262 loss: 0.023949\n",
            "[250/250] 24.95 sec(s) Train Acc: 0.935840 Loss: 0.003107 | Val Acc: 0.663848 loss: 0.024477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFYEAw22OvaA"
      },
      "source": [
        "train_val_x = np.concatenate((train_x, val_x), axis=0)\n",
        "train_val_y = np.concatenate((train_y, val_y), axis=0)\n",
        "train_val_set = ImgDataset(train_val_x, train_val_y, train_transform)\n",
        "train_val_loader = DataLoader(train_val_set, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "335nHbPlmzto"
      },
      "source": [
        "得到好的參數後，我們使用 training set 和 validation set 共同訓練（資料量變多，模型效果較好）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "465ao4qKOzGd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ddbdf44-1f90-4e1d-b187-772961b65d62"
      },
      "source": [
        "model_best = Classifier().cuda()\n",
        "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
        "optimizer = torch.optim.Adam(model_best.parameters(), lr=0.001) # optimizer 使用 Adam\n",
        "num_epoch = 250\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    epoch_start_time = time.time()\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "\n",
        "    model_best.train()\n",
        "    for i, data in enumerate(train_val_loader):\n",
        "        optimizer.zero_grad()\n",
        "        train_pred = model_best(data[0].cuda())\n",
        "        batch_loss = loss(train_pred, data[1].cuda())\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "        #將結果 print 出來\n",
        "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n",
        "      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
        "      train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/250] 30.41 sec(s) Train Acc: 0.174413 Loss: 0.036677\n",
            "[002/250] 30.43 sec(s) Train Acc: 0.243156 Loss: 0.035034\n",
            "[003/250] 30.33 sec(s) Train Acc: 0.263538 Loss: 0.033710\n",
            "[004/250] 30.26 sec(s) Train Acc: 0.302422 Loss: 0.031588\n",
            "[005/250] 30.24 sec(s) Train Acc: 0.318366 Loss: 0.030930\n",
            "[006/250] 30.27 sec(s) Train Acc: 0.344465 Loss: 0.030215\n",
            "[007/250] 30.27 sec(s) Train Acc: 0.366952 Loss: 0.029285\n",
            "[008/250] 30.27 sec(s) Train Acc: 0.388914 Loss: 0.028162\n",
            "[009/250] 30.34 sec(s) Train Acc: 0.410048 Loss: 0.027250\n",
            "[010/250] 30.72 sec(s) Train Acc: 0.423436 Loss: 0.026222\n",
            "[011/250] 30.75 sec(s) Train Acc: 0.439606 Loss: 0.025336\n",
            "[012/250] 30.85 sec(s) Train Acc: 0.453069 Loss: 0.024851\n",
            "[013/250] 30.53 sec(s) Train Acc: 0.466005 Loss: 0.024317\n",
            "[014/250] 30.46 sec(s) Train Acc: 0.465253 Loss: 0.023916\n",
            "[015/250] 30.37 sec(s) Train Acc: 0.479317 Loss: 0.023367\n",
            "[016/250] 30.35 sec(s) Train Acc: 0.492253 Loss: 0.022897\n",
            "[017/250] 30.40 sec(s) Train Acc: 0.500376 Loss: 0.022575\n",
            "[018/250] 30.32 sec(s) Train Acc: 0.501655 Loss: 0.022222\n",
            "[019/250] 30.42 sec(s) Train Acc: 0.511582 Loss: 0.022017\n",
            "[020/250] 30.44 sec(s) Train Acc: 0.514140 Loss: 0.021788\n",
            "[021/250] 30.46 sec(s) Train Acc: 0.516170 Loss: 0.021604\n",
            "[022/250] 30.48 sec(s) Train Acc: 0.531513 Loss: 0.021035\n",
            "[023/250] 30.47 sec(s) Train Acc: 0.537455 Loss: 0.020838\n",
            "[024/250] 30.70 sec(s) Train Acc: 0.546255 Loss: 0.020407\n",
            "[025/250] 30.73 sec(s) Train Acc: 0.551444 Loss: 0.020171\n",
            "[026/250] 30.48 sec(s) Train Acc: 0.555129 Loss: 0.020021\n",
            "[027/250] 30.38 sec(s) Train Acc: 0.557912 Loss: 0.019968\n",
            "[028/250] 30.45 sec(s) Train Acc: 0.566561 Loss: 0.019557\n",
            "[029/250] 30.30 sec(s) Train Acc: 0.569720 Loss: 0.019398\n",
            "[030/250] 30.37 sec(s) Train Acc: 0.569043 Loss: 0.019211\n",
            "[031/250] 30.38 sec(s) Train Acc: 0.579122 Loss: 0.019042\n",
            "[032/250] 30.36 sec(s) Train Acc: 0.585138 Loss: 0.018737\n",
            "[033/250] 30.49 sec(s) Train Acc: 0.590854 Loss: 0.018568\n",
            "[034/250] 30.34 sec(s) Train Acc: 0.591757 Loss: 0.018361\n",
            "[035/250] 30.36 sec(s) Train Acc: 0.594991 Loss: 0.018156\n",
            "[036/250] 30.37 sec(s) Train Acc: 0.603715 Loss: 0.017934\n",
            "[037/250] 30.45 sec(s) Train Acc: 0.604994 Loss: 0.017629\n",
            "[038/250] 30.52 sec(s) Train Acc: 0.603791 Loss: 0.017793\n",
            "[039/250] 30.37 sec(s) Train Acc: 0.615373 Loss: 0.017492\n",
            "[040/250] 30.43 sec(s) Train Acc: 0.614546 Loss: 0.017261\n",
            "[041/250] 30.50 sec(s) Train Acc: 0.614997 Loss: 0.017126\n",
            "[042/250] 30.49 sec(s) Train Acc: 0.621540 Loss: 0.016958\n",
            "[043/250] 30.53 sec(s) Train Acc: 0.628309 Loss: 0.016785\n",
            "[044/250] 30.53 sec(s) Train Acc: 0.628911 Loss: 0.016571\n",
            "[045/250] 30.47 sec(s) Train Acc: 0.642148 Loss: 0.016235\n",
            "[046/250] 30.51 sec(s) Train Acc: 0.639816 Loss: 0.016178\n",
            "[047/250] 30.46 sec(s) Train Acc: 0.640569 Loss: 0.016306\n",
            "[048/250] 30.48 sec(s) Train Acc: 0.648616 Loss: 0.015912\n",
            "[049/250] 30.45 sec(s) Train Acc: 0.649368 Loss: 0.015700\n",
            "[050/250] 30.46 sec(s) Train Acc: 0.651399 Loss: 0.015614\n",
            "[051/250] 30.35 sec(s) Train Acc: 0.657942 Loss: 0.015337\n",
            "[052/250] 30.40 sec(s) Train Acc: 0.661778 Loss: 0.015246\n",
            "[053/250] 30.36 sec(s) Train Acc: 0.661778 Loss: 0.015199\n",
            "[054/250] 30.27 sec(s) Train Acc: 0.669299 Loss: 0.014982\n",
            "[055/250] 30.30 sec(s) Train Acc: 0.662304 Loss: 0.015053\n",
            "[056/250] 30.27 sec(s) Train Acc: 0.676294 Loss: 0.014727\n",
            "[057/250] 30.33 sec(s) Train Acc: 0.678700 Loss: 0.014545\n",
            "[058/250] 30.27 sec(s) Train Acc: 0.675842 Loss: 0.014516\n",
            "[059/250] 30.37 sec(s) Train Acc: 0.685394 Loss: 0.014474\n",
            "[060/250] 30.43 sec(s) Train Acc: 0.680656 Loss: 0.014358\n",
            "[061/250] 30.36 sec(s) Train Acc: 0.686974 Loss: 0.014105\n",
            "[062/250] 30.29 sec(s) Train Acc: 0.688628 Loss: 0.014013\n",
            "[063/250] 30.33 sec(s) Train Acc: 0.697353 Loss: 0.013741\n",
            "[064/250] 30.28 sec(s) Train Acc: 0.701038 Loss: 0.013624\n",
            "[065/250] 30.27 sec(s) Train Acc: 0.703219 Loss: 0.013520\n",
            "[066/250] 30.27 sec(s) Train Acc: 0.704874 Loss: 0.013246\n",
            "[067/250] 30.32 sec(s) Train Acc: 0.712921 Loss: 0.013091\n",
            "[068/250] 30.27 sec(s) Train Acc: 0.715253 Loss: 0.013108\n",
            "[069/250] 30.38 sec(s) Train Acc: 0.715328 Loss: 0.012956\n",
            "[070/250] 30.41 sec(s) Train Acc: 0.717133 Loss: 0.012835\n",
            "[071/250] 30.36 sec(s) Train Acc: 0.725782 Loss: 0.012612\n",
            "[072/250] 30.43 sec(s) Train Acc: 0.721345 Loss: 0.012619\n",
            "[073/250] 30.44 sec(s) Train Acc: 0.725933 Loss: 0.012511\n",
            "[074/250] 30.34 sec(s) Train Acc: 0.720442 Loss: 0.012486\n",
            "[075/250] 30.42 sec(s) Train Acc: 0.729317 Loss: 0.012235\n",
            "[076/250] 30.41 sec(s) Train Acc: 0.733378 Loss: 0.012122\n",
            "[077/250] 30.63 sec(s) Train Acc: 0.734582 Loss: 0.012064\n",
            "[078/250] 30.53 sec(s) Train Acc: 0.734958 Loss: 0.011985\n",
            "[079/250] 30.38 sec(s) Train Acc: 0.738418 Loss: 0.011749\n",
            "[080/250] 30.39 sec(s) Train Acc: 0.742855 Loss: 0.011544\n",
            "[081/250] 30.37 sec(s) Train Acc: 0.744209 Loss: 0.011600\n",
            "[082/250] 30.49 sec(s) Train Acc: 0.750526 Loss: 0.011253\n",
            "[083/250] 30.38 sec(s) Train Acc: 0.751053 Loss: 0.011418\n",
            "[084/250] 30.39 sec(s) Train Acc: 0.750602 Loss: 0.011116\n",
            "[085/250] 30.38 sec(s) Train Acc: 0.753234 Loss: 0.011185\n",
            "[086/250] 30.36 sec(s) Train Acc: 0.761432 Loss: 0.010880\n",
            "[087/250] 30.37 sec(s) Train Acc: 0.759477 Loss: 0.010940\n",
            "[088/250] 30.42 sec(s) Train Acc: 0.769103 Loss: 0.010575\n",
            "[089/250] 30.40 sec(s) Train Acc: 0.762109 Loss: 0.010609\n",
            "[090/250] 30.37 sec(s) Train Acc: 0.769705 Loss: 0.010539\n",
            "[091/250] 30.45 sec(s) Train Acc: 0.766998 Loss: 0.010531\n",
            "[092/250] 30.41 sec(s) Train Acc: 0.776173 Loss: 0.010157\n",
            "[093/250] 30.39 sec(s) Train Acc: 0.776625 Loss: 0.010261\n",
            "[094/250] 30.44 sec(s) Train Acc: 0.769329 Loss: 0.010415\n",
            "[095/250] 30.47 sec(s) Train Acc: 0.781288 Loss: 0.009918\n",
            "[096/250] 30.42 sec(s) Train Acc: 0.782717 Loss: 0.009886\n",
            "[097/250] 30.43 sec(s) Train Acc: 0.781814 Loss: 0.009921\n",
            "[098/250] 30.38 sec(s) Train Acc: 0.779859 Loss: 0.009902\n",
            "[099/250] 30.42 sec(s) Train Acc: 0.787605 Loss: 0.009755\n",
            "[100/250] 30.38 sec(s) Train Acc: 0.790614 Loss: 0.009542\n",
            "[101/250] 30.47 sec(s) Train Acc: 0.792569 Loss: 0.009426\n",
            "[102/250] 30.68 sec(s) Train Acc: 0.793472 Loss: 0.009314\n",
            "[103/250] 30.48 sec(s) Train Acc: 0.799338 Loss: 0.009129\n",
            "[104/250] 30.39 sec(s) Train Acc: 0.791441 Loss: 0.009416\n",
            "[105/250] 30.37 sec(s) Train Acc: 0.801519 Loss: 0.008958\n",
            "[106/250] 30.28 sec(s) Train Acc: 0.803400 Loss: 0.009070\n",
            "[107/250] 30.22 sec(s) Train Acc: 0.803776 Loss: 0.008904\n",
            "[108/250] 30.22 sec(s) Train Acc: 0.798210 Loss: 0.009126\n",
            "[109/250] 30.23 sec(s) Train Acc: 0.810770 Loss: 0.008632\n",
            "[110/250] 30.32 sec(s) Train Acc: 0.809266 Loss: 0.008775\n",
            "[111/250] 30.32 sec(s) Train Acc: 0.809040 Loss: 0.008634\n",
            "[112/250] 30.45 sec(s) Train Acc: 0.812726 Loss: 0.008563\n",
            "[113/250] 30.41 sec(s) Train Acc: 0.817088 Loss: 0.008412\n",
            "[114/250] 30.28 sec(s) Train Acc: 0.811297 Loss: 0.008564\n",
            "[115/250] 30.49 sec(s) Train Acc: 0.820999 Loss: 0.008189\n",
            "[116/250] 30.45 sec(s) Train Acc: 0.822202 Loss: 0.008261\n",
            "[117/250] 30.40 sec(s) Train Acc: 0.825211 Loss: 0.007985\n",
            "[118/250] 30.54 sec(s) Train Acc: 0.823782 Loss: 0.007890\n",
            "[119/250] 30.46 sec(s) Train Acc: 0.826940 Loss: 0.007967\n",
            "[120/250] 30.39 sec(s) Train Acc: 0.827693 Loss: 0.007876\n",
            "[121/250] 30.45 sec(s) Train Acc: 0.830626 Loss: 0.007758\n",
            "[122/250] 30.36 sec(s) Train Acc: 0.832506 Loss: 0.007764\n",
            "[123/250] 30.41 sec(s) Train Acc: 0.828670 Loss: 0.007734\n",
            "[124/250] 30.31 sec(s) Train Acc: 0.833559 Loss: 0.007520\n",
            "[125/250] 30.36 sec(s) Train Acc: 0.833860 Loss: 0.007671\n",
            "[126/250] 30.32 sec(s) Train Acc: 0.836718 Loss: 0.007496\n",
            "[127/250] 30.37 sec(s) Train Acc: 0.837244 Loss: 0.007559\n",
            "[128/250] 30.29 sec(s) Train Acc: 0.841381 Loss: 0.007222\n",
            "[129/250] 30.23 sec(s) Train Acc: 0.838748 Loss: 0.007366\n",
            "[130/250] 30.19 sec(s) Train Acc: 0.841155 Loss: 0.007183\n",
            "[131/250] 30.27 sec(s) Train Acc: 0.844540 Loss: 0.007159\n",
            "[132/250] 30.28 sec(s) Train Acc: 0.842133 Loss: 0.007098\n",
            "[133/250] 30.38 sec(s) Train Acc: 0.850933 Loss: 0.006950\n",
            "[134/250] 30.34 sec(s) Train Acc: 0.845066 Loss: 0.007138\n",
            "[135/250] 30.42 sec(s) Train Acc: 0.846119 Loss: 0.007130\n",
            "[136/250] 30.46 sec(s) Train Acc: 0.847924 Loss: 0.006936\n",
            "[137/250] 30.33 sec(s) Train Acc: 0.855671 Loss: 0.006701\n",
            "[138/250] 30.46 sec(s) Train Acc: 0.851459 Loss: 0.006795\n",
            "[139/250] 30.40 sec(s) Train Acc: 0.850782 Loss: 0.006811\n",
            "[140/250] 30.42 sec(s) Train Acc: 0.853640 Loss: 0.006601\n",
            "[141/250] 30.38 sec(s) Train Acc: 0.853941 Loss: 0.006689\n",
            "[142/250] 30.37 sec(s) Train Acc: 0.861387 Loss: 0.006375\n",
            "[143/250] 30.45 sec(s) Train Acc: 0.858228 Loss: 0.006565\n",
            "[144/250] 30.49 sec(s) Train Acc: 0.858078 Loss: 0.006523\n",
            "[145/250] 30.36 sec(s) Train Acc: 0.853189 Loss: 0.006548\n",
            "[146/250] 30.28 sec(s) Train Acc: 0.862891 Loss: 0.006452\n",
            "[147/250] 30.32 sec(s) Train Acc: 0.861312 Loss: 0.006423\n",
            "[148/250] 30.19 sec(s) Train Acc: 0.861838 Loss: 0.006315\n",
            "[149/250] 30.25 sec(s) Train Acc: 0.864320 Loss: 0.006072\n",
            "[150/250] 30.34 sec(s) Train Acc: 0.868833 Loss: 0.006061\n",
            "[151/250] 30.36 sec(s) Train Acc: 0.863718 Loss: 0.006116\n",
            "[152/250] 30.39 sec(s) Train Acc: 0.863418 Loss: 0.006203\n",
            "[153/250] 30.48 sec(s) Train Acc: 0.864997 Loss: 0.006128\n",
            "[154/250] 30.37 sec(s) Train Acc: 0.867780 Loss: 0.006049\n",
            "[155/250] 30.44 sec(s) Train Acc: 0.871014 Loss: 0.005880\n",
            "[156/250] 30.44 sec(s) Train Acc: 0.874925 Loss: 0.005776\n",
            "[157/250] 30.35 sec(s) Train Acc: 0.869359 Loss: 0.005984\n",
            "[158/250] 30.42 sec(s) Train Acc: 0.871766 Loss: 0.005948\n",
            "[159/250] 30.38 sec(s) Train Acc: 0.874022 Loss: 0.005799\n",
            "[160/250] 30.41 sec(s) Train Acc: 0.878460 Loss: 0.005765\n",
            "[161/250] 30.37 sec(s) Train Acc: 0.873120 Loss: 0.005805\n",
            "[162/250] 30.37 sec(s) Train Acc: 0.877708 Loss: 0.005699\n",
            "[163/250] 30.32 sec(s) Train Acc: 0.871766 Loss: 0.005728\n",
            "[164/250] 30.22 sec(s) Train Acc: 0.877256 Loss: 0.005644\n",
            "[165/250] 30.20 sec(s) Train Acc: 0.874398 Loss: 0.005734\n",
            "[166/250] 30.18 sec(s) Train Acc: 0.880490 Loss: 0.005542\n",
            "[167/250] 30.22 sec(s) Train Acc: 0.878836 Loss: 0.005562\n",
            "[168/250] 30.20 sec(s) Train Acc: 0.884401 Loss: 0.005417\n",
            "[169/250] 30.27 sec(s) Train Acc: 0.885454 Loss: 0.005313\n",
            "[170/250] 30.17 sec(s) Train Acc: 0.879813 Loss: 0.005566\n",
            "[171/250] 30.41 sec(s) Train Acc: 0.883123 Loss: 0.005506\n",
            "[172/250] 30.37 sec(s) Train Acc: 0.882145 Loss: 0.005419\n",
            "[173/250] 30.35 sec(s) Train Acc: 0.885755 Loss: 0.005305\n",
            "[174/250] 30.48 sec(s) Train Acc: 0.885529 Loss: 0.005163\n",
            "[175/250] 30.38 sec(s) Train Acc: 0.884251 Loss: 0.005283\n",
            "[176/250] 30.36 sec(s) Train Acc: 0.891095 Loss: 0.005082\n",
            "[177/250] 30.40 sec(s) Train Acc: 0.883950 Loss: 0.005232\n",
            "[178/250] 30.36 sec(s) Train Acc: 0.887936 Loss: 0.005162\n",
            "[179/250] 30.44 sec(s) Train Acc: 0.887259 Loss: 0.005231\n",
            "[180/250] 30.38 sec(s) Train Acc: 0.891471 Loss: 0.005055\n",
            "[181/250] 30.39 sec(s) Train Acc: 0.888914 Loss: 0.005016\n",
            "[182/250] 30.32 sec(s) Train Acc: 0.893727 Loss: 0.004946\n",
            "[183/250] 30.41 sec(s) Train Acc: 0.886808 Loss: 0.005179\n",
            "[184/250] 30.31 sec(s) Train Acc: 0.894103 Loss: 0.004829\n",
            "[185/250] 30.18 sec(s) Train Acc: 0.895984 Loss: 0.004764\n",
            "[186/250] 30.09 sec(s) Train Acc: 0.894856 Loss: 0.004919\n",
            "[187/250] 30.30 sec(s) Train Acc: 0.891622 Loss: 0.004997\n",
            "[188/250] 30.26 sec(s) Train Acc: 0.897488 Loss: 0.004718\n",
            "[189/250] 30.29 sec(s) Train Acc: 0.896059 Loss: 0.004821\n",
            "[190/250] 30.37 sec(s) Train Acc: 0.894254 Loss: 0.004681\n",
            "[191/250] 30.31 sec(s) Train Acc: 0.896059 Loss: 0.004850\n",
            "[192/250] 30.27 sec(s) Train Acc: 0.898090 Loss: 0.004783\n",
            "[193/250] 30.33 sec(s) Train Acc: 0.901098 Loss: 0.004563\n",
            "[194/250] 30.29 sec(s) Train Acc: 0.897413 Loss: 0.004718\n",
            "[195/250] 30.25 sec(s) Train Acc: 0.896360 Loss: 0.004683\n",
            "[196/250] 30.28 sec(s) Train Acc: 0.897789 Loss: 0.004757\n",
            "[197/250] 30.26 sec(s) Train Acc: 0.905535 Loss: 0.004378\n",
            "[198/250] 30.25 sec(s) Train Acc: 0.900647 Loss: 0.004580\n",
            "[199/250] 30.28 sec(s) Train Acc: 0.902527 Loss: 0.004497\n",
            "[200/250] 30.24 sec(s) Train Acc: 0.901324 Loss: 0.004564\n",
            "[201/250] 30.12 sec(s) Train Acc: 0.906363 Loss: 0.004453\n",
            "[202/250] 30.07 sec(s) Train Acc: 0.907265 Loss: 0.004463\n",
            "[203/250] 30.18 sec(s) Train Acc: 0.904106 Loss: 0.004446\n",
            "[204/250] 30.17 sec(s) Train Acc: 0.904257 Loss: 0.004413\n",
            "[205/250] 30.10 sec(s) Train Acc: 0.905836 Loss: 0.004289\n",
            "[206/250] 30.11 sec(s) Train Acc: 0.905385 Loss: 0.004373\n",
            "[207/250] 30.05 sec(s) Train Acc: 0.907115 Loss: 0.004232\n",
            "[208/250] 30.15 sec(s) Train Acc: 0.906438 Loss: 0.004316\n",
            "[209/250] 30.13 sec(s) Train Acc: 0.909146 Loss: 0.004252\n",
            "[210/250] 30.15 sec(s) Train Acc: 0.910800 Loss: 0.004215\n",
            "[211/250] 30.38 sec(s) Train Acc: 0.905385 Loss: 0.004370\n",
            "[212/250] 30.19 sec(s) Train Acc: 0.910424 Loss: 0.004146\n",
            "[213/250] 30.14 sec(s) Train Acc: 0.909823 Loss: 0.004209\n",
            "[214/250] 30.21 sec(s) Train Acc: 0.910725 Loss: 0.004237\n",
            "[215/250] 30.15 sec(s) Train Acc: 0.911402 Loss: 0.004184\n",
            "[216/250] 30.11 sec(s) Train Acc: 0.908845 Loss: 0.004335\n",
            "[217/250] 30.22 sec(s) Train Acc: 0.912229 Loss: 0.004095\n",
            "[218/250] 30.69 sec(s) Train Acc: 0.907491 Loss: 0.004193\n",
            "[219/250] 30.79 sec(s) Train Acc: 0.905912 Loss: 0.004258\n",
            "[220/250] 30.31 sec(s) Train Acc: 0.909221 Loss: 0.004200\n",
            "[221/250] 30.28 sec(s) Train Acc: 0.914561 Loss: 0.003888\n",
            "[222/250] 30.48 sec(s) Train Acc: 0.910951 Loss: 0.004052\n",
            "[223/250] 30.58 sec(s) Train Acc: 0.911928 Loss: 0.004108\n",
            "[224/250] 30.45 sec(s) Train Acc: 0.917795 Loss: 0.003913\n",
            "[225/250] 30.44 sec(s) Train Acc: 0.909146 Loss: 0.004159\n",
            "[226/250] 30.37 sec(s) Train Acc: 0.913207 Loss: 0.003968\n",
            "[227/250] 30.56 sec(s) Train Acc: 0.914486 Loss: 0.003879\n",
            "[228/250] 30.20 sec(s) Train Acc: 0.914561 Loss: 0.003870\n",
            "[229/250] 30.39 sec(s) Train Acc: 0.917795 Loss: 0.003808\n",
            "[230/250] 30.77 sec(s) Train Acc: 0.914937 Loss: 0.003939\n",
            "[231/250] 30.55 sec(s) Train Acc: 0.919073 Loss: 0.003784\n",
            "[232/250] 30.36 sec(s) Train Acc: 0.920954 Loss: 0.003763\n",
            "[233/250] 30.39 sec(s) Train Acc: 0.921104 Loss: 0.003805\n",
            "[234/250] 30.41 sec(s) Train Acc: 0.918096 Loss: 0.003872\n",
            "[235/250] 30.60 sec(s) Train Acc: 0.914862 Loss: 0.003999\n",
            "[236/250] 30.44 sec(s) Train Acc: 0.916140 Loss: 0.003916\n",
            "[237/250] 30.43 sec(s) Train Acc: 0.915388 Loss: 0.003839\n",
            "[238/250] 30.49 sec(s) Train Acc: 0.918773 Loss: 0.003862\n",
            "[239/250] 30.44 sec(s) Train Acc: 0.917419 Loss: 0.003838\n",
            "[240/250] 30.46 sec(s) Train Acc: 0.918697 Loss: 0.003819\n",
            "[241/250] 30.45 sec(s) Train Acc: 0.921931 Loss: 0.003645\n",
            "[242/250] 30.44 sec(s) Train Acc: 0.917043 Loss: 0.003713\n",
            "[243/250] 30.44 sec(s) Train Acc: 0.917494 Loss: 0.003843\n",
            "[244/250] 30.39 sec(s) Train Acc: 0.921931 Loss: 0.003633\n",
            "[245/250] 30.57 sec(s) Train Acc: 0.920653 Loss: 0.003760\n",
            "[246/250] 30.42 sec(s) Train Acc: 0.920277 Loss: 0.003641\n",
            "[247/250] 30.67 sec(s) Train Acc: 0.921405 Loss: 0.003740\n",
            "[248/250] 30.81 sec(s) Train Acc: 0.921104 Loss: 0.003637\n",
            "[249/250] 30.53 sec(s) Train Acc: 0.918547 Loss: 0.003821\n",
            "[250/250] 30.48 sec(s) Train Acc: 0.925842 Loss: 0.003532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0FxpcXJDkuK"
      },
      "source": [
        "torch.save(model_best.state_dict(), 'model_best5.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLOU55rZDnbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87132e8e-c869-4727-a458-9897db35e8ed"
      },
      "source": [
        "state_dict = torch.load('model_best5.pth')\n",
        "print(state_dict.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['cnn.0.weight', 'cnn.0.bias', 'cnn.1.weight', 'cnn.1.bias', 'cnn.1.running_mean', 'cnn.1.running_var', 'cnn.1.num_batches_tracked', 'cnn.4.weight', 'cnn.4.bias', 'cnn.5.weight', 'cnn.5.bias', 'cnn.5.running_mean', 'cnn.5.running_var', 'cnn.5.num_batches_tracked', 'cnn.9.weight', 'cnn.9.bias', 'cnn.10.weight', 'cnn.10.bias', 'cnn.10.running_mean', 'cnn.10.running_var', 'cnn.10.num_batches_tracked', 'cnn.13.weight', 'cnn.13.bias', 'cnn.14.weight', 'cnn.14.bias', 'cnn.14.running_mean', 'cnn.14.running_var', 'cnn.14.num_batches_tracked', 'cnn.18.weight', 'cnn.18.bias', 'cnn.19.weight', 'cnn.19.bias', 'cnn.19.running_mean', 'cnn.19.running_var', 'cnn.19.num_batches_tracked', 'fc.0.weight', 'fc.0.bias', 'fc.2.weight', 'fc.2.bias', 'fc.4.weight', 'fc.4.bias', 'fc.6.weight', 'fc.6.bias', 'fc.7.weight', 'fc.7.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkF_mypmDpY6"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}